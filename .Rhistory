testing <-Wage[-inTrain,];
dim(training);dim(testing)
featurePlot(x=training,[,c("age","education","jobclass")],
y= training$wage,
plot="pairs")
featurePlot(x=training,[,c("age","education","jobclass")],
y= training$wage,
plot="pairs")
featurePlot(x=training,[,c("age","education","jobclass")],y= training$wage,
plot="pairs")
raining,[,c("age","education","jobclass")],y= training$wage,plot="pairs")
featurePlot(x=training,[,c("age","education","jobclass")],y= training$wage,plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],y= training$wage,plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],y= training$wage,plot="pairs")
qplot(age,wage,data=training)
dev.off()
qplot(age,wage,data=training)
qplot(age,wage,colour=education,data=training)
modFit<- train(wage ~ age + jobclass + education,
method = "lm",data=training)
fidMod <- modFit$finalModel
print(modFit)
modFit<- train(wage ~ age + jobclass + education,
method = "lm",data=training)
fidMod <- modFit$finalModel
print(modFit)
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
modFitAll<- train(wage ~.,data=training,method="lm")
pred <- predict(modFitAll,testing)
qplot(wage,pred,data = testing)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
testIndex = createDataPartition(diagnosis, p=0.50, list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
# No relation between the outcome and other variables
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
# Step-like pattern -> 4 categories
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
# Another way
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
# There is a step-like pattern in the plot of outcome versus index
# in the training set that isn't explained by any of the predictor
# variables so there may be a variable missing.
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
(Superplasticizer, data=training)
ggplot(data=training, aes(x=Superplasticizer)) + geom_histogram() + theme_bw()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelfit)
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelfit)
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelFit)
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelFit)
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
print(attributes(training))
preProc <- preProcess(training[58:69], , method = "pca", thresh = 0.80 )
print(preProc)
print(preProc$dim)
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1 # Non-PCA Accuracy: 0.65
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2 # PCA Accuracy: 0.72
rm(list = ls())
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Extract all the IL variables
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelfit)
# Extract all the IL variables
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
rm(list = ls())
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Extract all the IL variables
subtrain = training[,substr(names(training), 1, 3) == 'IL_']
r = preProcess(subtrain, method = "pca", thresh = 0.8)
rtrain = predict(r, subtrain)
rtrain$diagnosis = training$diagnosis
View(predictors)
modelfit = train(rtrain$diagnosis ~ ., method = 'glm', data = rtrain)
summary(modelfit)
modelfit = train(rtrain$diagnosis ~ , method = 'glm', data = rtrain)
summary(modelfit)
library(ElemStatLearn)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(quantmod)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google",from = from.dat, to = to.dat)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="yahoo",from = from.dat, to = to.dat)
library(quantmod)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="yahoo",from = from.dat, to = to.dat)
head(GOOG)
mGoog <- to.monthly(GOOG)
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency = 12)
plot(ts1,xlab="Years+1",ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
# Load library
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
# Load csv
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw)
dim(testRaw)
trainRaw <- read.csv("./raw_data/pml-training.csv")
testRaw <- read.csv("./raw_data/pml-testing.csv")
# Load csv
trainRaw <- read.csv("C:\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
testRaw <- read.csv("C:\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
dim(trainRaw)
dim(testRaw)
trainRaw <- read.csv("C:\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
trainRaw <- read.csv("C:\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
trainRaw <- read.csv("C:\\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
trainRaw <- read.csv("C:\\Users\\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
trainRaw <- read.csv("C:\\Users\\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis\raw_data/pml-training.csv")
trainRaw <- read.csv("raw_data/pml-training.csv")
trainRaw <- read.csv("raw_data\pml-training.csv")
# Set working directory
setwd("C:\Users\david.LIZZY\Documents\GitHub\Human-Activity-Recognition-Analysis")
# Set working directory
setwd("C:/Users/david.LIZZY/Documents/GitHub/Human-Activity-Recognition-Analysis")
trainRaw <- read.csv("raw_data\pml-training.csv")
trainRaw <- read.csv("raw_data/pml-training.csv")
testRaw <- read.csv("raw_data/pml-training.csv")
dim(trainRaw)
dim(testRaw)
sum(complete.cases(trainRaw))
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0]
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0]
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]
set.seed(22519) # For reproducibile purpose
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
modelRf
predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)
accuracy <- postResample(predictRf, testData$classe)
accuracy
oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])
oose
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color"
# Correlation Matrix Visualization
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color")
# Decision Tree Visualization
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel) # fast plot
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color")
head(trainRaw)
head(testRaw)
# Load library
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
install.packages("rattle", dependencies = T)
# Load library
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
set.seed(12345)
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
set.seed(12345)
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
# Partion the training set into two
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
# Remove the first column of the myTraining data set
myTraining <- myTraining[c(-1)]
# Clean variables with more than 60% NA
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
trainingV3 <- trainingV3[ , -j]
}
}
}
}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
# Transform the myTesting and testing data sets
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
# Coerce the data into the same type
for (i in 1:length(testing) ) {
for(j in 1:length(myTraining)) {
if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
class(testing[j]) <- class(myTraining[i])
}
}
}
# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
# Prediction with Decision Trees
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
# Prediction with Random Forests
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
# Prediction with Random Forests
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
plot(modFitB1)
# Random Forest Confusion Matrix
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
# Prediction with Generalized Boosted Regression
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
trControl = fitControl,
verbose = FALSE)
gbmFinMod1 <- gbmFit1$finalModel
gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest
plot(gbmFit1, ylim=c(0.9, 1))
# Predicting Results on the Test Data
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
# Predicting Results on the Test Data
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
install.packages("rmarkdown")
install.packages("rmarkdown")
---
title: "report"
author: "Long Gu"
date: "September 19, 2018"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Partion the training set into two
```{r}
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```
## Remove NearZeroVariance variables
```{r}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]
nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
```
## Remove the first column of the myTraining data set
```{r}
myTraining <- myTraining[c(-1)]
```
## Clean variables with more than 60% NA
```{r}
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
trainingV3 <- trainingV3[ , -j]
}
}
}
}
```
## Set back to the original variable name
```{r}
myTraining <- trainingV3
rm(trainingV3)
```
## Transform the myTesting and testing data sets
```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining
dim(myTesting)
```
